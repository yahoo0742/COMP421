# -*- coding: utf-8 -*-
"""Copy of COMP421_assignment01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v5jWQ2RHvtpDuKFu4-dPGwCAW5wDFGxW
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
print(tf.__version__)
from tensorflow import keras
import numpy as np
import time
import datetime
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import axes3d
# %reload_ext tensorboard
!pip install scprep tasklogger
import scprep
!rm -rf ./logs/

def gendata(instancenum):
  featuredim = 3 # x, y, z
  shape = [instancenum, featuredim]
  mean = 0
  stddev = 1
  xyznormaldistributedinstances = tf.random.normal(shape, mean, stddev, tf.float32, seed=1) # [[x1,y1,z1], [x2,y2,z2], ...]
  # radius = (x*x + y*y +z*z) ^ (1/2)
  xxyyzz = np.square(xyznormaldistributedinstances) # [[x1*x1,y1*y1,z1*z1], [x2*x2,y2*y2,z2*z2], ... ]
  xxyyzzsum = np.sum(xxyyzz, 1) # [x1y1z1_sum, x2y2z2_sum, ...]
  radiusofinstances = np.sqrt(xxyyzzsum) # radius = xyz_sum^(1/2), [x1y1z1_sum^(1/2), x2y2z2_sum^(1/2), ...]

  # normalize (length = 1) all data
  invradius = np.divide(1, radiusofinstances)
  tileR = np.tile(invradius,(3,1))
  s = np.multiply(np.ndarray.transpose(tileR), xyznormaldistributedinstances)
  normalizedinstances = xyznormaldistributedinstances * invradius[:,None]
  return (xyznormaldistributedinstances, normalizedinstances)


TRAIN_BUF = 100 # 1000
TEST_BUF = 30 # 300
BATCH_SIZE = 10 #100

traininstances, normalizedtrainInstances = gendata(TRAIN_BUF)
testinstances, normalizedtestInstances = gendata(TEST_BUF)

# x, s = gendata(TRAIN_BUF)
# xt, st = gendata(TEST_BUF)
train_data = tf.concat([normalizedtrainInstances,normalizedtrainInstances], 1)  # tf.concat([s,s],1)
test_data = tf.concat([normalizedtestInstances,normalizedtestInstances], 1) # tf.concat([st,st],1)

#tf.print(tf.shape(train_data))
#np.set_printoptions(precision=3)
#print(x)
#print(sradius)

train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(TRAIN_BUF).batch(BATCH_SIZE)
test_dataset = tf.data.Dataset.from_tensor_slices(test_data).shuffle(TEST_BUF).batch(BATCH_SIZE)

scprep.plot.rotate_scatter3d(normalizedtrainInstances, c=normalizedtrainInstances[:, 0], title="Generated data")

class CIAE(tf.keras.Model):
  def __init__(self, latent_dim):
    super(CIAE, self).__init__()
    self.latent_dim = latent_dim
    self.encoder = tf.keras.Sequential(
        [
          tf.keras.layers.InputLayer(input_shape=(3,)),
          tf.keras.layers.Dense(units=10, activation='relu'),
          tf.keras.layers.Dense(units=10, activation='relu'),
          tf.keras.layers.Dense(latent_dim, activation='linear'),
        ]
    )

    self.decoder = tf.keras.Sequential(
        [
          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
          tf.keras.layers.Dense(units=16, activation='relu'), #'relu'
          tf.keras.layers.Dense(units=16, activation='relu'),
          tf.keras.layers.Dense(units=3, activation='linear'),
        ]
    )

  def sample(self, eps=None):
    print("====sample", time.time())
    if eps is None:
      eps = tf.random.normal(shape=(1000, self.latent_dim))
    return self.decoder(eps)

  def addnoise(self, mean, stdev):
    eps = stdev * tf.random.normal(shape=mean.shape)
    return eps + mean

# this cell does the actual optimization
optimizer = tf.keras.optimizers.Adam(1e-3)
epochs = 200
latent_dim = 1
model = CIAE(latent_dim)
d_rate_bits = 25. # bits per dimension (10 is good)
d_rate_nats = d_rate_bits * tf.math.log(2.0)

print("rates: ", tf.math.log(2.0))
stdev = 0.1
alpha = 0.01

#define metrics and create summary writer
train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
val_loss = tf.keras.metrics.Mean()
current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
train_log_dir = 'logs/gradient_tape/' + current_time + '/train'
val_log_dir = 'logs/gradient_tape/' + current_time + '/val'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
val_summary_writer = tf.summary.create_file_writer(val_log_dir)

@tf.function
def compute_loss(model, thedata, d_rate_nats):
  x, y = tf.split(thedata, 2, 1)
  mean = model.encoder(x)

  z = model.addnoise(mean, stdev)
  tf.print("z: ",z, "mean ",mean)
  yhat = model.decoder(z)
  q = tf.keras.losses.MSE(yhat, y)

  reconstruction_error = tf.reduce_mean(q)
  tf.print("q: ",reconstruction_error)

  maxinfo = tf.math.log(tf.reduce_mean(tf.math.square(z), 0)/(stdev*stdev))
  maxinfo = tf.reduce_mean(maxinfo)
  infoerror = tf.math.abs(d_rate_nats - maxinfo)
  total_error = reconstruction_error + alpha * infoerror
  return total_error, maxinfo, reconstruction_error
#return reconstruction_error, maxinfo. reconstruction_error

@tf.function
def train_iteration(model, thedata, desiredrate, optimizer):
  with tf.GradientTape() as tape:
    t_loss, rate, reconE = compute_loss(model, thedata, desiredrate)
  # compute gradients and update variables
  gradients = tape.gradient(t_loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return t_loss

# run training
for epoch in range(1, epochs + 1):
  # start_time = time.time()
  for train_x in train_dataset:
    t_loss = train_iteration(model, train_x, d_rate_nats, optimizer)
    train_loss(t_loss)
  with train_summary_writer.as_default():
    tf.summary.scalar('t_loss', train_loss.result(), step=epoch)
  # end_time = time.time()

  for test_x in test_dataset:
    v_loss, rate, reconE = compute_loss(model, test_x, d_rate_nats)
    val_loss(v_loss)
  with val_summary_writer.as_default():
    tf.summary.scalar('v_loss', val_loss.result(), step=epoch)

  if epoch % 500 == 0 or epoch == 1:
    t_error = train_loss.result()
    v_error = val_loss.result()
    print('Epoch: {}, Train set error: {} Val set error: {}, rate: {} reconE: {}'.format(epoch, t_error, v_error, rate, reconE))
    #print('Time elapse for current epoch : {}'.format(epoch, t_error, v_error, rate, reconE))
  
print("layers ",model.get_layer(None, 0).get_config())